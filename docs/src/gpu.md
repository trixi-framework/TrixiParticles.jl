# [GPU Support](@id gpu_support)

GPU support is still an experimental feature that is actively being worked on.
As of now, the [`WeaklyCompressibleSPHSystem`](@ref), the [`TotalLagrangianSPHSystem`](@ref)
and the [`BoundarySPHSystem`](@ref) are supported on GPUs.
We have tested this on GPUs by Nvidia, AMD and Apple.
Note that Apple GPUs currently require the development version of
[GPUCompiler.jl](https://github.com/JuliaGPU/GPUCompiler.jl) and most of them
do not support `Float64`.
See [below on how to run single precision simulations](@ref single_precision).

To run a simulation on a GPU, we need to use the [`FullGridCellList`](@ref)
as cell list for the [`GridNeighborhoodSearch`](@ref).
This cell list requires a bounding box for the domain, unlike the default cell list, which
uses an unbounded domain.
For simulations that are bounded by a closed tank, we can simply use the boundary
of the tank to obtain the bounding box as follows.
```jldoctest gpu; output=false, setup=:(using TrixiParticles; trixi_include(@__MODULE__, joinpath(examples_dir(), "fluid", "hydrostatic_water_column_2d.jl"), sol=nothing))
min_corner = minimum(tank.boundary.coordinates, dims=2)
max_corner = maximum(tank.boundary.coordinates, dims=2)
cell_list = TrixiParticles.PointNeighbors.FullGridCellList(; min_corner, max_corner)

# output
PointNeighbors.FullGridCellList{PointNeighbors.DynamicVectorOfVectors{Int32, Matrix{Int32}, Vector{Int32}, Base.RefValue{Int32}}, Nothing, SVector{2, Float64}, SVector{2, Float64}}(Vector{Int32}[], nothing, [-0.12500000000000003, -0.12500000000000003], [1.125, 1.125])
```

We then need to pass this cell list to the neighborhood search and the neighborhood search
to the [`Semidiscretization`](@ref).
```jldoctest gpu; output=false
semi = Semidiscretization(fluid_system, boundary_system,
                          neighborhood_search=GridNeighborhoodSearch{2}(; cell_list))

# output
┌──────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Semidiscretization                                                                               │
│ ══════════════════                                                                               │
│ #spatial dimensions: ………………………… 2                                                                │
│ #systems: ……………………………………………………… 2                                                                │
│ neighborhood search: ………………………… GridNeighborhoodSearch                                           │
│ total #particles: ………………………………… 636                                                              │
└──────────────────────────────────────────────────────────────────────────────────────────────────┘
```

At this point, we should run the simulation and make sure that it still works and that
the bounding box is large enough.
For some simulations where particles move outside the initial tank coordinates,
for example when the tank is not closed or when the tank is moving, an appropriate
bounding box has to be specified.

Then, we only need to specify the data type that is used for the simulation.
On an Nvidia GPU, we specify:
```julia
using CUDA
ode = semidiscretize(semi, tspan, data_type=CuArray)
```
On an AMD GPU, we use:
```julia
using AMDGPU
ode = semidiscretize(semi, tspan, data_type=ROCArray)
```
Now, we can run the simulation as usual.
All data is transferred to the GPU during initialization and all loops over particles
and their neighbors will be executed on the GPU as kernels generated by KernelAbstractions.jl.
Data is only copied to the CPU for saving VTK files via the [`SolutionSavingCallback`](@ref).

## Run an existing example file on the GPU

The example file `examples/fluid/dam_break_2d_gpu.jl` demonstrates how to run an existing
example file on a GPU.
It first loads the variables from the example file `examples/fluid/dam_break_2d.jl`
without running the simulation by overwriting the line that starts the simulation
with `trixi_include(..., sol=nothing)`.
Then, a GPU-compatible neighborhood search is defined, and the original example file
is included with the new neighborhood search.
This requires the assignments `neighborhood_search = ...` and `data_type = ...` in the
original example file.
Note that in `examples/fluid/dam_break_2d.jl`, we specifically set `data_type=nothing`, even though
this is the default value, so that we can use `trixi_include` to replace this value.

To now run this simulation on a GPU, all we have to do is change `data_type` to the
array type of the installed GPU.
For example, we can run this simulation on an Nvidia GPU as follows.
```julia
using CUDA
trixi_include(joinpath(examples_dir(), "fluid", "dam_break_2d_gpu.jl"), data_type=CuArray)
```

## [Single precision simulations](@id single_precision)

All features that currently support GPUs can also be used with single precision.
This is orders of magnitude faster on most GPUs and required to run code
on Metal (Apple GPUs).

To run a simulation with single precision, all `Float64` literals in an example file
have to be changed to `Float32` (e.g. `0.0` to `0.0f0`).
TrixiParticles provides a function to do this conveniently:
```@docs
    trixi_include_changeprecision
```

All we have to do to run the previous example with single precision is the following.
```julia
using CUDA
trixi_include_changeprecision(Float32,
                              joinpath(examples_dir(), "fluid", "dam_break_2d_gpu.jl"),
                              data_type=CuArray)
```
